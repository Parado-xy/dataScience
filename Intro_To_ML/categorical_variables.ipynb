{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables.\n",
    "\n",
    "There's a lot of non-numeric data out there, here's how to use it for Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <bold>Categorical variable</bold> takes ony a fixed number of values.\n",
    "\n",
    "- Consider a survey that asks how often you eat breakfast and provides four options: 'Never', 'Rarely','Most days','Every Day'. In this case, the data ic categorical because responses fall in a fixed set of catehories. \n",
    "\n",
    "You'll get an error if you try to plug these variables into most machine learning models in python without preprocessing them. \n",
    "We'll go over 3 approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Drop Categorical Variables :** The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns do not contain useful information. \n",
    "\n",
    "2. **Ordinal Encoding :** This assigns each unique value to a different integer. \n",
    "<img src = '../images/ordinal_encoding.png'>\n",
    "\n",
    "This approach assumes an ordering of the categories: 'Never' : 0 > 'Rarely' 1 > 'Most Days' 2 > 'Every Day' 3.\n",
    "\n",
    "This assumption makes sense in this example because there is an indisputable ranking to the categories. Not all categorical variables have a clear orddering in the values, but we refer to those that do as <bold>Ordinal Variables</bold>. For tree based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables. \n",
    "\n",
    "3. **One Hot Encoding :** This creates new columns indicating the presence or absence of each possible value in the original data. \n",
    "<img src = '../images/one_hot_encoding.png'> \n",
    "\n",
    "In the Original Dataset, 'Color' is a Categorical Variable with 3 Categories : 'Red', 'Yellow', and 'Green'. The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Whenever the original value was 'Red', we put a 1 in the 'Red' column; if the original value was 'Yellow', we put 1 in the 'Yellow' column and so on. \n",
    "\n",
    "In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect the approach to work particularly well if there's no clear ordering in the categorical data (e.g, 'Red' is neither more, nor less than 'Yellow'). We refer to categorical variables without an intrinsic ranking as **nominal variables**.\n",
    "\n",
    "One-Hot encoding generally does not perform well if the categorical variables takes on a large number of values (i.e, you generally won't use it for variables taking more than 15 different values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark. \n",
    "Let's bench mark these things. We'll use the melbourn housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_data = pd.read_csv(filepath_or_buffer = '../datasets/melb_data.csv')\n",
    "melb_data.dropna(inplace = True) # Fastest way to \"Clean\" the data.\n",
    "y = melb_data.Price\n",
    "X = melb_data.drop(['Date','Address','CouncilArea','Suburb','SellerG'], axis = 1)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SellerG was dropped because it contains 246 unique entries, that's a lot to be honest.\n",
    "# I don't need the Date it was sold... i think. There are only 58 unique dates, so...? Maybe we should come back to that.\n",
    "# Suburb was removed because there are 314 unique values, we also have region name already, which kinda seems like a generalization. \n",
    "# I don't need the address.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Categorical Variables: \n",
      " ['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "# Let's obtain a list of all categorical columns in the dataset. \n",
    "categorical = (X_train.dtypes == 'object')\n",
    "\n",
    "categorical = list(categorical[categorical].index)\n",
    "print(f' Categorical Variables: \\n {categorical}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Funcion to look at the quality of each approach. \n",
    "We'll define a function that uses the mean absolute error to compare the 3 different approaches of dealing with categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_approach(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    MAE = mean_absolute_error(y_test, preds)\n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating approach 1, drop dtypes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE FOR APPROACH 1 (DROP CATEGORICAL VARIABLES)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(976.2410587475787)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude = 'object')\n",
    "drop_X_test = X_test.select_dtypes(exclude = 'object')\n",
    "\n",
    "print('MAE FOR APPROACH 1 (DROP CATEGORICAL VARIABLES)')\n",
    "score_approach(drop_X_train, drop_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating approach 2, Ordinal Encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE FOR APPROACH 2 (ORDINAL ENCODING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(797.5960038734668)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make copy to avoid changing the original Dataset.\n",
    "\n",
    "label_X_train = X_train.copy()\n",
    "label_X_test = X_test.copy()\n",
    "\n",
    "# Apply ordinal Encoder. \n",
    "OE = OrdinalEncoder()\n",
    "label_X_train[categorical] = OE.fit_transform(label_X_train[categorical]) \n",
    "label_X_test[categorical] = OE.transform(label_X_test[categorical]) \n",
    "\n",
    "print('MAE FOR APPROACH 2 (ORDINAL ENCODING)')\n",
    "score_approach(label_X_train, label_X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating approach 3, One-Hot Encoding.\n",
    "\n",
    "We use the OneHotEncoder class from scikit-learn to get one-hot-encodings. There are a number of parameters that can be used to customize its behaviour.\n",
    "\n",
    "- We set `handle_unknown = ignore` to avoid errors when the validation data contains classes that aren't represented in the training data. \n",
    "- Setting `sparse = False` ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE FOR APPROACH 3 (ONE-HOT ENCODING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(831.4351387992252)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply one hot encoder to each column with categorical data. \n",
    "OH = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "OH_cols_train =  pd.DataFrame(OH.fit_transform(X_train[categorical]))\n",
    "OH_cols_test = pd.DataFrame(OH.transform(X_test[categorical]))\n",
    "\n",
    "# One Hot encoding removed the index; put it back. \n",
    "OH_cols_test.index = X_test.index\n",
    "OH_cols_train.index = X_train.index\n",
    "\n",
    "# Remove Categorical Columns, we'll replace with one hot encoding. \n",
    "num_X_train = X_train.select_dtypes(exclude = 'object')\n",
    "num_X_test = X_test.select_dtypes(exclude = 'object')\n",
    "\n",
    "# Concatenate One_Hot and Numerical features. \n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1)\n",
    "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis = 1)\n",
    "\n",
    "# Ensure all columns have string types. \n",
    "OH_X_train.columns = OH_X_train.columns.astype('str') \n",
    "OH_X_test.columns = OH_X_test.columns.astype('str')\n",
    "\n",
    "print('MAE FOR APPROACH 3 (ONE-HOT ENCODING)')\n",
    "score_approach(OH_X_train, OH_X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which approach is best?\n",
    "\n",
    "Well, here, it is clear that Ordinal Encoding did best and Dropping The columns did worse but i believe it varies from dataset to dataset so check it out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
