{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression. \n",
    "Ridge regresssion is also a regression model so it uses the same formula as the OLS regression model. In ridge regression though, the coefficients (w) are chosen not only so they can predict well on traning data, but also to fit an additional constraint. We also want  the magnitude of coefficients to be as small as possible; inother words, all entries of w should be close to zero. Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well. This is an example of regularization. Regularization means expliciting a model to avoid overfitting. Ridge regression uses L2 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Melbourn housing data.\n",
    "melb_data = pd.read_csv('../datasets/melb_data.csv')\n",
    "# Drop missing values. \n",
    "melb_data.dropna(axis = 0, inplace = True)\n",
    "# Get the prices. \n",
    "y = melb_data['Price']\n",
    "# Get the numerical features\n",
    "X = melb_data.select_dtypes(exclude = 'object').drop(['Price'], axis = 1, )\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2 Score : 0.59\n",
      "Test R^2 Score : 0.64\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge().fit(X_train, y_train) # Instantiate the model and perform the fit step.\n",
    "# Evaluate the model.\n",
    "print(f'Training R^2 Score : {ridge.score(X_train, y_train):.2f}' ) \n",
    "print(f'Test R^2 Score : {ridge.score(X_test, y_test):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test R<sup>2</sup> scores are the same for the LinearRegression model and the Ridge regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT's EXPLANATION.\n",
    "\n",
    "The `Ridge` model in scikit-learn is a type of linear regression model that includes regularization. Regularization is a technique used to prevent overfitting by adding a penalty to the size of the coefficients. Ridge regression is also known as L2 regularization.\n",
    "\n",
    "Here are the key aspects of the `Ridge` model:\n",
    "\n",
    "\n",
    "1. **Regularization Strength (`alpha`)**: The parameter `alpha` controls the amount of regularization applied to the model. A higher value of `alpha` increases the regularization strength, which can help reduce overfitting but may also lead to underfitting. Conversely, a lower value of `alpha` reduces the regularization effect, making the model closer to ordinary least squares regression.\n",
    "\n",
    "2. **Implementation**: The `Ridge` model can be implemented in scikit-learn as follows:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.linear_model import Ridge\n",
    "   \n",
    "   # Create a Ridge regression model\n",
    "   ridge = Ridge(alpha=1.0)\n",
    "   \n",
    "   # Fit the model to the data\n",
    "   ridge.fit(X_train, y_train)\n",
    "   \n",
    "   # Make predictions\n",
    "   y_pred = ridge.predict(X_test)\n",
    "   ```\n",
    "\n",
    "3. **Attributes**:\n",
    "   - `coef_`: The coefficients (weights) of the linear model.\n",
    "   - `intercept_`: The intercept of the linear model.\n",
    "\n",
    "4. **Methods**:\n",
    "   - `fit(X, y)`: Fit the model to the training data.\n",
    "   - `predict(X)`: Predict target values for new data.\n",
    "   - `score(X, y)`: Return the coefficient of determination \\( R^2 \\) of the prediction.\n",
    "\n",
    "5. **Advantages**:\n",
    "   - Can handle multicollinearity (correlated features) better than ordinary least squares regression.\n",
    "   - Helps in preventing overfitting by introducing a penalty for large coefficients.\n",
    "\n",
    "6. **Limitations**:\n",
    "   - Requires tuning of the regularization parameter `alpha`.\n",
    "   - May not perform well if the true relationship between the features and target is highly non-linear.\n",
    "\n",
    "In summary, `Ridge` is a powerful tool for linear regression that incorporates regularization to improve generalization and robustness, particularly in the presence of multicollinearity among features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression.\n",
    "An alternative for regularizing linear regression is `Lasso`. As with ridge regression, using Lasso also restricts coefficients to be close to zero, but in a slightly different way, called L1 regularization. The consequences of L1 regularization is that when using the lasso, some coefficients are <i>exactly</i> zero.  This means some features are enirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interprete, and can reveal the most important features of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2 Score : 0.59\n",
      "Test R^2 Score : 0.64\n",
      "Number of feature used 12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "lasso = Lasso(alpha = 0.1, max_iter = 10000).fit(X_train, y_train) # Instantiate and fit the model. \n",
    "print(f'Training R^2 Score : {lasso.score(X_train, y_train):.2f}')\n",
    "print(f'Test R^2 Score : {lasso.score(X_test, y_test):.2f}')\n",
    "print(f'Number of feature used {np.sum(lasso.coef_ != 0)}') # Sum Up the number of Non-Zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It achieved the same metrics as the Ridge model. I hink it didn't drop anything because the number of features is low. \n",
    "\n",
    "\n",
    "`Also note that Ridge regression is usually the first choice between the 2. `"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
